# AMPLE - Implementation
## Vulnerability Detection with Graph Simplification and Enhanced Graph Representation Learning

## Introduction
Prior studies have demonstrated the effectiveness of Deep Learning (DL) in automated software vulnerability detection. Graph Neural Networks (GNNs) have proven effective in learning the graph representations of source code and are commonly adopted by existing DL-based vulnerability detection methods. However, the existing methods are still limited by the fact that GNNs are essentially difficult to handle the connections between long-distance nodes in a code structure graph. Besides, they do not well exploit the multiple types of edges in a code structure graph (such as edges representing data flow and control flow). Consequently, despite achieving state-of-the-art performance, the existing GNN-based methods tend to fail to capture global information (ie, long-range dependencies among nodes) of code graphs. 

To mitigate these issues, in this paper, we propose a novel vulnerability detection framework with grAph siMplification and enhanced graph rePresentation LEarning, named AMPLE. AMPLE mainly contains two parts: 1) graph simplification, which aims at reducing the distances between nodes by shrinking the node sizes of code structure graphs; 2) enhanced graph representation learning, which involves one edge-aware graph convolutional network module for fusing heterogeneous edge information into node representations and one kernel-scaled representation moule for well capturing the relations between distant graph nodes. Experiments on three public benchmark datasets show that AMPLE outperforms the state-of-the-art methods by 0.39%-35.32% and 7.64%-199.81% with respect to the accuracy and F1 score metrics, respectively. The results demonstrate the effectiveness of AMPLE in learning global information of code graphs for vulnerability detection.

## Dataset
To investigate the effectiveness of AMPLE, we adopt three vulnerability datasets from these paper: 
* Fan et al. [1]: <https://drive.google.com/file/d/1-0VhnHBp9IGh90s2wCNjeCMuy70HPl8X/view?usp=sharing>
* Reveal [2]: https://drive.google.com/drive/folders/1KuIYgFcvWUXheDhT--cBALsfy1I4utOyF
* FFMPeg+Qemu [3]: https://drive.google.com/file/d/1x6hoF7G-tSYxg8AFybggypLZgMGDNHfF

## Requirement
Our code is based on Python3 (>= 3.7). There are a few dependencies to run the code. The major libraries are listed as follows:
* torch  (==1.9.0)
* dgl  (==0.7.2)
* numpy  (==1.22.3)
* sklearn  (==0.0)
* pandas  (==1.4.1)
* tqdm

**Default settings in AMPLE**:
* Training configs: 
    * batch_size = 64, lr = 0.0001, epoch = 100, patience = 20
    * opt ='RAdam', weight_decay=1e-6

## Preprocessing
We use Joern to generate the code structure graph and we provide a compiled version of joern [here](https://zenodo.org/record/7323504#.Y3OQL3ZByUk). It should be noted that the AST and graphs generated by different versions of Joern may have significant differences. So if using the newer versions of Joern to generate code structure graph, the model may have a different performance compared with the results we reported in the paper.

After parsing the functions with joern, the code for graph construction and simplification is under the ```data_processing\``` folder. ```data_processing\word2vec.py``` is used to train word2vec model. We also provide our trained word2vec model [here](https://zenodo.org/record/7333062#.Y3c5SHZByUk).

## Running the model
The model implementation code is under the ``` AMPLE_code\``` folder. The model can be runned from ```AMPLE_code\main.py```.

## Attention weight
We provide all the attention weights learned by our proposed model AMPLE for the test samples. Each dataset corresponds to a json file under ```attention weight\``` folder.

## Experiment results
### PR-AUC & MCC && G-measure && T-test
<center>Table 1. Experiment results for Reveal and AMPLE. "*" denotes sttistical significance in comparision to Reveal in terms of accuracy and F1 score (i.e., two-sided t-test with p-value < 0.05).</center>


<table align="center">
    <tr>
        <td>Dataset</td> 
        <td>Metrics</td>
         <td>Reveal</td> 
         <td>AMPLE</td>
         <!-- <td>Improment</td> -->

   </tr>
    <tr>
        <td rowspan="7">FFmpeg+Qemu</td>  
        <td>Accuracy</td> 
        <td>0.6107</td>    
        <td><b>0.6216</b></td>
    </tr>
    <tr>
        <td>Precision</td>    
        <td>0.5550</td>    
        <td><b>0.5564</b></td>  
    </tr>
    <tr>
        <td>Recall</td>    
        <td>0.7070</td>    
        <td><b>0.8399</b></td>  
    </tr>
    <tr>
        <td>F1 score</td>    
        <td>0.6219</td>    
        <td><b>0.6694 *</b></td>
    </tr>
    <tr>
        <td><b>PR-AUC</b></td>    
        <td>0.6972</td>    
        <td><b>0.7347</b></td>  
    </tr>
    <tr>
       <td><b>MCC</b></td>
       <td>0.2398</td>    
        <td><b>0.2995</b></td>  
    </tr>
    <tr>
       <td><b>G-measure</b></td>
       <td>0.6264</td>    
        <td><b>0.6836</b></td>  
    </tr>
    <tr>
        <td rowspan="7">Reveal</td>  
        <td>Accuracy</td> 
        <td>0.8177</td>    
        <td><b>0.9271 *</b></td>  
    </tr>
    <tr>
        <td>Precision</td>    
        <td>0.3155</td>    
        <td><b>0.5106</b></td>  
    </tr>
    <tr>
        <td>Recall</td>    
        <td><b>0.6114</b></td>    
        <td>0.4615</td>  
    </tr>
    <tr>
        <td>F1 score</td>    
        <td>0.4162</td>    
        <td><b>0.4848 *</b></td>  
    </tr>
    <tr>
        <td><b>PR-AUC</b></td>    
        <td>0.4841</td>    
        <td><b>0.5061</b></td>  
    </tr>
    <tr>
       <td><b>MCC</b></td>
       <td>0.3457</td>    
        <td><b>0.4464</b></td>  
    </tr>
    <tr>
       <td><b>G-measure</b></td>
       <td>0.4392</td>    
        <td><b>0.4854</b></td>  
    </tr>
    <tr>
        <td rowspan="7">Fan et al.</td>   
        <td>Accuracy</td>    
        <td>0.8714</td>    
        <td><b>0.9314 *</b></td>  
    </tr>
    <tr>
       <td>Precision</td>
       <td>0.1722</td>    
        <td><b>0.2998</b></td>  
    </tr>
    <tr>
       <td>Recall</td>
       <td>0.3404</td>    
        <td><b>0.3458</b></td>  
    </tr>
    <tr>
       <td>F1 score</td>
       <td>0.2287</td>    
        <td><b>0.3211 *</b></td>  
    </tr>
    <tr>
       <td><b>PR-AUC</b></td>
       <td>0.2748</td>    
        <td><b>0.3383</b></td>  
    </tr>
    <tr>
       <td><b>MCC</b></td>
       <td>0.1783</td>    
        <td><b>0.2860</b></td>  
    </tr>    
    <tr>
       <td><b>G-measure</b></td>
       <td>0.2421</td>    
        <td><b>0.322</b></td>  
    </tr>			
</table>

<center>Table 2. The p-value of two-sided t-test results between AMPLE and Reveal in terms of accuracy and F1 score.</center>

<table align="center">
<tr>
    <td></td>
    <td>FFmpeg+Qemu</td>
    <td>Reveal</td>
    <td>Fan et al.</td>
</tr>
<tr>
    <td>Accuracy</td>
    <td>0.41</td>
    <td><b>4.67e-12</b></td>
    <td><b>1.00e-4</b></td>
</tr>
<tr>
    <td>F1 score</td>
    <td><b>1.00e-3</b></td>
    <td><b>4.55e-6</b></td>
    <td><b>1.41e-9</b></td>
</tr>
</table>


<!-- #### T-test

<table>
<tr>
    <td> </td>
</tr>

</table> -->

## References
[1] Jiahao Fan, Yi Li, Shaohua Wang, and Tien Nguyen. 2020. A C/C++ Code Vulnerability Dataset with Code Changes and CVE Summaries. In The 2020 International Conference on Mining Software Repositories (MSR). IEEE.

[2] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray. 2020. Deep Learning based Vulnerability Detection: Are We There Yet? arXiv preprint arXiv:2009.07235 (2020).

[3] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. In Advances in Neural Information Processing Systems. 10197â€“10207.

[4] M. Fu and C. Tantithamthavorn. 2022. Linevul: A transformer-based line-level vulnerability prediction. In The 2022 International Conference on Mining Software Repositories (MSR). IEEE.

